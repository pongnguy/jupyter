{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Problem Formulation\nI formulate this task as an extractive question answering problem, such as SQuAD.  \nGiven a question and context, the model is trained to find the answer spans in the context.\n\nTherefore, I use sentiment as question, text as context, selected_text as answer.\n- Question: sentiment\n- Context: text\n- Answer: selected_text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameters & Options ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Hyperparameters\nbatch_size = 16 # batch size\nlr = 5e-5 # learning rate\nepochs = 2 # number of epochs\nmax_seq_len = 128 # max sequence length\ndoc_stride = 64 # document stride\n\n# Options\ncross_validation = True # whether to use cross-validation\nK = 2 # number of CV splits\npost_processing = True # whether to use post-processing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Packages","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n### Load Data","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"pd_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\npd_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np_train = np.array(pd_train)\nnp_test = np.array(pd_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-fold Split\nSplit the data into K folds for cross validation. Use the fixed random seed for reproducibility.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Given a data size, return the train/valid indicies for K splits.\ndef split_data(num_examples, K):\n    np.random.seed(0)\n    idx = np.arange(num_examples)\n    np.random.shuffle(idx)\n    \n    boundary = num_examples // K\n    splits = [{} for _ in range(K)]\n    for i in range(K):\n        splits[i]['valid_idx'] = idx[i*boundary:(i+1)*boundary]\n        splits[i]['train_idx'] = np.concatenate((idx[:i*boundary], idx[(i+1)*boundary:]))\n\n        valid = np_train[splits[i]['valid_idx']]\n        d = {'neutral':0, 'positive':0, 'negative':0}\n        for line in valid:\n            d[line[-1]] += 1\n        print(d)\n        \n    return splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"splits = split_data(len(np_train), K)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert Data to SQuAD-style\nIn this part, I convert the data into SQuAD-style.  \nSince I think most of the errors in the dataset are irreducible, I do not use additional preprocessing methods to handle them.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert data to SQuAD-style\ndef convert_data(data, directory, filename):\n    def find_all(input_str, search_str):\n        l1 = []\n        length = len(input_str)\n        index = 0\n        while index < length:\n            i = input_str.find(search_str, index)\n            if i == -1:\n                return l1\n            l1.append(i)\n            index = i + 1\n        return l1\n    \n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    \n    for line in data:\n        paragraphs = []\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(context) != str:\n            print(context, type(context))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer})\n        qas.append({'question': '[MASK]', 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context, 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    with open(os.path.join(directory, filename), 'w') as outfile:\n        json.dump(output, outfile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert k-fold train data\nfor i, split in enumerate(splits):\n    data = np_train[split['train_idx']]\n    directory = 'split_' + str(i+1)\n    filename = 'train.json'\n    convert_data(data, directory, filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert original train/test data\ndata = np_train\ndirectory = 'original'\nfilename = 'train.json'\nconvert_data(data, directory, filename)\n\ndata = np_test\nfilename = 'test.json'\nconvert_data(data, directory, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finetuning\nInstall the pytorch-transformers package (v2.5.1) of [huggingface](https://github.com/huggingface/transformers).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /kaggle/input/pytorchtransformers/transformers-2.5.1; pip install .","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-Validation\nFinetune QA models for cross-validation.","execution_count":null},{"metadata":{"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"def run_script(train_file, predict_file, batch_size=16, lr=5e-5, epochs=2, max_seq_len=128, doc_stride=64):\n    !python /kaggle/input/pytorchtransformers/transformers-2.5.1/examples/run_squad.py \\\n    --model_type distilbert \\\n    --model_name_or_path distilbert-base-uncased \\\n    --cache_dir /kaggle/input/cached-distilbert-base-uncased/cache \\\n    --do_lower_case \\\n    --do_train \\\n    --do_eval \\\n    --train_file=$train_file \\\n    --predict_file=$predict_file \\\n    --overwrite_cache \\\n    --learning_rate=$lr \\\n    --num_train_epochs=$epochs \\\n    --max_seq_length=$max_seq_len \\\n    --doc_stride=$doc_stride \\\n    --output_dir ./results \\\n    --overwrite_output_dir \\\n    --per_gpu_eval_batch_size=$batch_size \\\n    --per_gpu_train_batch_size=$batch_size \\\n    --save_steps=100000","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!mkdir results","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"if cross_validation:\n    for i in range(1, K+1):\n        train_file = \"split_\" + str(i) + \"/train.json\"\n        predict_file = \"original/train.json\"\n        run_script(train_file, predict_file, batch_size, lr, epochs, max_seq_len, doc_stride)\n        !mv \"results/predictions_.json\" \"results/predictions_\"$i\".json\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation\nCalculate train/valid scores.","execution_count":null},{"metadata":{"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def evaluate(splits, np_train, post_processing=False):\n    K = len(splits)\n    predictions = [json.load(open('results/predictions_' + str(i+1) + '.json', 'r')) for i in range(K)]\n\n    train_score = [{'neutral':[], 'positive':[], 'negative':[], 'total':[]} for _ in range(K+1)]\n    valid_score = [{'neutral':[], 'positive':[], 'negative':[], 'total':[]} for _ in range(K+1)]\n\n    for train_idx, line in enumerate(np_train):\n        text_id = line[0]\n        text = line[1]\n        answer = line[2]\n        sentiment = line[-1]\n\n        if type(text) != str:\n            continue\n\n        for i, prediction in enumerate(predictions):\n            if text_id not in prediction:\n                print('key error:', text_id)\n                continue\n            else:\n                if post_processing and (sentiment == 'neutral' or len(text.split()) <= 0): # post-processing\n                    score = jaccard(answer, text)\n                else:\n                    score = jaccard(answer, prediction[text_id])\n\n                if train_idx in splits[i]['valid_idx']:\n                    valid_score[i][sentiment].append(score)\n                    valid_score[i]['total'].append(score)\n                    valid_score[K][sentiment].append(score)\n                    valid_score[K]['total'].append(score)\n\n                else:\n                    train_score[i][sentiment].append(score)\n                    train_score[i]['total'].append(score)\n                    train_score[K][sentiment].append(score)\n                    train_score[K]['total'].append(score)\n\n    for i, score_dict in enumerate([train_score, valid_score]):\n        if i == 0:\n            print('train score \\n')\n        else:\n            print('valid score \\n')\n        for j in range(K+1):\n            for sentiment in ['neutral', 'positive', 'negative', 'total']:\n                score = np.array(score_dict[j][sentiment])\n                if j < K:\n                    print('split', j+1)\n                else:\n                    print('all data')\n                print(sentiment + ' - ' + str(len(score)) + ' examples, average score: ' + str(score.mean()))\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if cross_validation:\n    evaluate(splits, np_train, post_processing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test\nFinetune a model for the test.","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train_file = \"original/train.json\"\npredict_file = \"original/test.json\"\nrun_script(train_file, predict_file, batch_size, lr, epochs, max_seq_len, doc_stride)\n!mv results/predictions_.json results/test_predictions.json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Copy predictions to submission file.\npredictions = json.load(open('results/test_predictions.json', 'r'))\nsubmission = pd.read_csv(open('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv', 'r'))\nfor i in range(len(submission)):\n    id_ = submission['textID'][i]\n    if post_processing and (pd_test['sentiment'][i] == 'neutral' or len(pd_test['text'][i].split()) <= 0): # post-processing\n        submission.loc[i, 'selected_text'] = pd_test['text'][i]\n    else:\n        submission.loc[i, 'selected_text'] = predictions[id_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Save the submission file.\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}