{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tX9nDQnr8AzT"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/6/6d/Nvidia_image_logo.svg\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# BERT Question Answering in TensorFlow with Mixed Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kL-6-WT78AzR"
   },
   "source": [
    "Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "FOa47jxd80bS",
    "outputId": "4c1db5fb-0dd8-45b3-d00b-bccfb1941e78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Loy_jvmr8AzT"
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "Bidirectional Embedding Representations from Transformers (BERT), is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. \n",
    "\n",
    "The original paper can be found here: https://arxiv.org/abs/1810.04805.\n",
    "\n",
    "NVIDIA's BERT is an optimized version of Google's official implementation, leveraging mixed precision arithmetic and tensor cores on V100 GPUS for faster training times while maintaining target accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXp2mMCx8AzU"
   },
   "source": [
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Inference on Question Answering (QA) task with BERT Large model\n",
    "- The use/download of fine-tuned NVIDIA BERT models from [NGC](https://ngc.nvidia.com)\n",
    "- Use of Mixed Precision models for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLlJiTQN8AzV"
   },
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiQ5qvJD8Azm"
   },
   "source": [
    "### Pre-Trained NVIDIA BERT TensorFlow Models on NGC\n",
    "\n",
    "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2019/03/18-ngc-software-stack-447x500.png\" style=\"width: 360px;\">\n",
    "\n",
    "We will be using the following configuration of BERT in this example:\n",
    "\n",
    "| **Model** | **Hidden layers** | **Hidden unit size** | **Attention heads** | **Feedforward filter size** | **Max sequence length** | **Parameters** |\n",
    "|:---------:|:----------:|:----:|:---:|:--------:|:---:|:----:|\n",
    "|BERTLARGE|24 encoder|1024| 16|4 x 1024|512|330M|\n",
    "\n",
    "**To do so, we will take advantage of the pre-trained models available on the [NGC Model Registry](https://ngc.nvidia.com/catalog/models).**\n",
    "\n",
    "Among the many configurations available we will download one of these two:\n",
    "\n",
    " - **bert_tf_v2_large_fp32_384**\n",
    "\n",
    " - **bert_tf_v2_large_fp16_384**\n",
    "\n",
    "which are trained on the [SQuaD 2.0 Dataset](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iJR47XD8Azg"
   },
   "source": [
    "We can choose the mixed precision model (which takes much less time to train than the fp32 version) without losing accuracy, with the following flag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT8mFmG51eUt"
   },
   "outputs": [],
   "source": [
    "use_mixed_precision_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C08Gf-PH8Azn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /workspace/bert/data/finetuned_model_fp16/bert_tf_v2_large_fp16_384.zip\r\n"
     ]
    }
   ],
   "source": [
    "if use_mixed_precision_model:\n",
    "    # bert_tf_v2_large_fp16_384\n",
    "    !mkdir -p /workspace/bert/data/finetuned_model_fp16\n",
    "    !wget -nc -q --show-progress -O /workspace/bert/data/finetuned_model_fp16/bert_tf_v2_large_fp16_384.zip \\\n",
    "    https://api.ngc.nvidia.com/v2/models/nvidia/bert_tf_v2_large_fp16_384/versions/1/zip\n",
    "    !unzip -n -d /workspace/bert/data/finetuned_model_fp16/ /workspace/bert/data/finetuned_model_fp16/bert_tf_v2_large_fp16_384.zip \n",
    "else:\n",
    "    # bert_tf_v2_large_fp32_384\n",
    "    !mkdir -p /workspace/bert/data/finetuned_model_fp32\n",
    "    !wget -nc -q --show-progress -O /workspace/bert/data/finetuned_model_fp32/bert_tf_v2_large_fp32_384.zip \\\n",
    "    https://api.ngc.nvidia.com/v2/models/nvidia/bert_tf_v2_large_fp32_384/versions/1/zip\n",
    "    !unzip -n -d /workspace/bert/data/finetuned_model_fp32/ /workspace/bert/data/finetuned_model_fp32/bert_tf_v2_large_fp32_384.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuE_gCBUp6uD"
   },
   "source": [
    "### NGC Model Scripts\n",
    "\n",
    "While we're at it, we'll also pull down some BERT helper scripts from the [NGC Model Scripts Registry](https://ngc.nvidia.com/catalog/model-scripts/nvidia:bert_for_tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kavDaBXpqd7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘bert_scripts.zip’ already there; not retrieving.\n",
      "Archive:  bert_scripts.zip\n"
     ]
    }
   ],
   "source": [
    "# Download BERT helper scripts\n",
    "!wget -nc --show-progress -O bert_scripts.zip \\\n",
    "     https://api.ngc.nvidia.com/v2/recipes/nvidia/bert_for_tensorflow/versions/1/zip\n",
    "!mkdir -p /workspace/bert\n",
    "!unzip -n -d /workspace/bert bert_scripts.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEs0P1C_RPIi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘/workspace/bert/config.qa/vocab.txt’ already there; not retrieving.\r\n"
     ]
    }
   ],
   "source": [
    "# Download BERT vocab file\n",
    "!mkdir -p /workspace/bert/config.qa\n",
    "!wget -nc https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt \\\n",
    "    -O /workspace/bert/config.qa/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2tAJ5TRRUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspace/bert/config.qa/bert_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspace/bert/config.qa/bert_config.json\n",
    "{\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 1024,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 4096,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamic JSON files based on user inputs\n",
    "def write_input_file(context, qinputs, predict_file):\n",
    "    # Remove quotes and new lines from text for valid JSON\n",
    "    context = context.replace('\"', '').replace('\\n', '')\n",
    "    # Create JSON dict to write\n",
    "    json_dict = {\n",
    "      \"data\": [\n",
    "        {\n",
    "          \"title\": \"BERT QA\",\n",
    "          \"paragraphs\": [\n",
    "            {\n",
    "              \"context\": context,\n",
    "              \"qas\": qinputs\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    # Write JSON to input file\n",
    "    with open(predict_file, 'w') as json_file:\n",
    "        import json\n",
    "        json.dump(json_dict, json_file, indent=2)\n",
    "    \n",
    "# Display Inference Results as HTML Table\n",
    "def display_results(predict_file, output_prediction_file):\n",
    "    import json\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    # Here we show only the prediction results, nbest prediction is also available in the output directory\n",
    "    results = \"\"\n",
    "    with open(predict_file, 'r') as query_file:\n",
    "        queries = json.load(query_file)\n",
    "        input_data = queries[\"data\"]\n",
    "        with open(output_prediction_file, 'r') as result_file:\n",
    "            data = json.load(result_file)\n",
    "            for entry in input_data:\n",
    "                for paragraph in entry[\"paragraphs\"]:\n",
    "                    for qa in paragraph[\"qas\"]:\n",
    "                        results += \"<tr><td>{}</td><td>{}</td><td>{}</td></tr>\".format(qa[\"id\"], qa[\"question\"], data[qa[\"id\"]])\n",
    "\n",
    "    display(HTML(\"<table><tr><th>Id</th><th>Question</th><th>Answer</th></tr>{}</table>\".format(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLdBPppf8AzV"
   },
   "source": [
    "## 3. BERT Inference: Question Answering\n",
    "\n",
    "We can run inference on a fine-tuned BERT model for tasks like Question Answering.\n",
    "\n",
    "Here we use a BERT model fine-tuned on a [SQuaD 2.0 Dataset](https://rajpurkar.github.io/SQuAD-explorer/) which contains 100,000+ question-answer pairs on 500+ articles combined with over 50,000 new, unanswerable questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-jHuLNk8AzW"
   },
   "source": [
    "### Paragraph and Queries\n",
    "\n",
    "In this example we will ask our BERT model questions related to the following paragraph:\n",
    "\n",
    "**The Apollo Program**\n",
    "_\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\"_\n",
    "\n",
    "  \n",
    "---\n",
    "\n",
    "The paragraph and the questions can be easily customized by changing the code below:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dr_eMAtfSN5R"
   },
   "outputs": [],
   "source": [
    "# Create BERT input file with (1) context and (2) questions to be answered based on that context\n",
    "predict_file = '/workspace/bert/config.qa/input.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LcOfv3dn8AzX",
    "outputId": "ae803153-071a-4d5e-82df-8f684c9d0ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspace/bert/config.qa/input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $predict_file\n",
    "{\"data\": \n",
    " [\n",
    "     {\"title\": \"Project Apollo\",\n",
    "      \"paragraphs\": [\n",
    "          {\"context\":\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\", \n",
    "           \"qas\": [\n",
    "               { \"question\": \"What project put the first Americans into space?\", \n",
    "                 \"id\": \"Q1\"\n",
    "               },\n",
    "               { \"question\": \"What program was created to carry out these projects and missions?\",\n",
    "                 \"id\": \"Q2\"\n",
    "               },\n",
    "               { \"question\": \"What year did the first manned Apollo flight occur?\",\n",
    "                 \"id\": \"Q3\"\n",
    "               },                \n",
    "               { \"question\": \"What President is credited with the notion of putting Americans on the moon?\",\n",
    "                 \"id\": \"Q4\"\n",
    "               },\n",
    "               { \"question\": \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\",\n",
    "                 \"id\": \"Q5\"\n",
    "               },\n",
    "               { \"question\": \"How long did Project Apollo run?\",\n",
    "                 \"id\": \"Q6\"\n",
    "               },               \n",
    "               { \"question\": \"What program helped develop space travel techniques that Project Apollo used?\",\n",
    "                 \"id\": \"Q7\"\n",
    "               },                \n",
    "               {\"question\": \"What space station supported three manned missions in 1973-1974?\",\n",
    "                 \"id\": \"Q8\"\n",
    "               }\n",
    "]}]}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTAgKSvG8Azj"
   },
   "source": [
    "To effectively evaluate the speedup of mixed precision try a bigger workload by uncommenting the following lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqYng4Fh6PrK"
   },
   "source": [
    "**TODO: Waiting on model scripts repo to fix Windows newlines in squad_download.sh**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMJ-3aXN8Azk"
   },
   "outputs": [],
   "source": [
    "#!bash /workspace/bert/data/squad/squad_download.sh\n",
    "#predict_file = '/workspace/bert/data/squad/v2.0/dev-v2.0.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNPDdF_f8Azq"
   },
   "source": [
    "## 4. Running Question/Answer Inference\n",
    "\n",
    "To run QA inference we will launch the script run_squad.py with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNA4ezvR8Azr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This specifies the model architecture.\n",
    "bert_config_file = '/workspace/bert/config.qa/bert_config.json'\n",
    "\n",
    "# The vocabulary file that the BERT model was trained on.\n",
    "vocab_file = '/workspace/bert/config.qa/vocab.txt'\n",
    "\n",
    "# Depending on the mixed precision flag we use different fine-tuned model\n",
    "if use_mixed_precision_model:\n",
    "    init_checkpoint = '/workspace/bert/data/finetuned_model_fp16/model.ckpt-8144'\n",
    "else:\n",
    "    init_checkpoint = '/workspace/bert/data/finetuned_model_fp32/model.ckpt-8144'\n",
    "\n",
    "# Create the output directory where all the results are saved.\n",
    "output_dir = '/workspace/bert/results'\n",
    "output_prediction_file = os.path.join(output_dir,'predictions.json')\n",
    "    \n",
    "# Whether to lower case the input - True for uncased models / False for cased models.\n",
    "do_lower_case = True\n",
    "  \n",
    "# Total batch size for predictions\n",
    "predict_batch_size = 8\n",
    "\n",
    "# Whether to run eval on the dev set.\n",
    "do_predict = True\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "doc_stride = 128\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "max_seq_length = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcC81ooQ8Azt"
   },
   "source": [
    "### 4a. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "No3_W3fd8Azt",
    "outputId": "552205fd-4e3e-48a8-898b-836412062d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-04 17:41:20.232821: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1004 17:41:21.340837 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/optimization.py:110: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1004 17:41:21.354933 140007069570880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/horovod-0.16.2-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:107: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "W1004 17:41:21.356337 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1409: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "W1004 17:41:21.356724 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1174: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1004 17:41:21.356826 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1174: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W1004 17:41:21.356929 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/modeling.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1004 17:41:21.357506 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1183: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W1004 17:41:21.442186 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1199: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1004 17:41:21.945942 140007069570880 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1004 17:41:21.946371 140007069570880 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f551bca6048>) includes params argument, but params are not passed to Estimator.\n",
      "I1004 17:41:21.947021 140007069570880 estimator.py:209] Using config: {'_model_dir': '/workspace/bert/results', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f55102d5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "I1004 17:41:21.947435 140007069570880 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
      "W1004 17:41:21.947723 140007069570880 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n",
      "W1004 17:41:21.947842 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:266: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1004 17:41:21.948863 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1112: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W1004 17:41:21.994768 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1354: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "I1004 17:41:21.994851 140007069570880 run_squad.py:1354] ***** Running predictions *****\n",
      "I1004 17:41:21.994913 140007069570880 run_squad.py:1355]   Num orig examples = 8\n",
      "I1004 17:41:21.995212 140007069570880 run_squad.py:1356]   Num split examples = 8\n",
      "I1004 17:41:21.995281 140007069570880 run_squad.py:1357]   Batch size = 8\n",
      "W1004 17:41:21.995372 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:731: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "I1004 17:41:21.995577 140007069570880 estimator.py:612] Could not find trained model in model_dir: /workspace/bert/results, running initialization to predict.\n",
      "W1004 17:41:22.015170 140007069570880 deprecation.py:323] From /workspace/bert/run_squad.py:776: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W1004 17:41:22.015273 140007069570880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W1004 17:41:22.016580 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:743: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W1004 17:41:22.019920 140007069570880 deprecation.py:323] From /workspace/bert/run_squad.py:750: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I1004 17:41:22.035059 140007069570880 estimator.py:1145] Calling model_fn.\n",
      "I1004 17:41:22.035180 140007069570880 tpu_estimator.py:2965] Running infer on CPU\n",
      "W1004 17:41:22.038202 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1004 17:41:22.039526 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/modeling.py:413: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1004 17:41:22.120327 140007069570880 deprecation.py:323] From /workspace/bert/modeling.py:675: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1004 17:41:26.304313 140007069570880 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:670: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "I1004 17:41:27.458134 140007069570880 estimator.py:1147] Done calling model_fn.\n",
      "W1004 17:41:27.706729 140007069570880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I1004 17:41:28.256394 140007069570880 monitored_session.py:240] Graph was finalized.\n",
      "2019-10-04 17:41:28.261712: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3198235000 Hz\n",
      "2019-10-04 17:41:28.262294: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f27410 executing computations on platform Host. Devices:\n",
      "2019-10-04 17:41:28.262310: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-10-04 17:41:28.264070: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-04 17:41:28.720664: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffe6b0 executing computations on platform CUDA. Devices:\n",
      "2019-10-04 17:41:28.720686: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:41:28.720691: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:41:28.720696: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:41:28.720701: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:41:28.721833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:07:00.0\n",
      "2019-10-04 17:41:28.722693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:08:00.0\n",
      "2019-10-04 17:41:28.723557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:0e:00.0\n",
      "2019-10-04 17:41:28.724414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:0f:00.0\n",
      "2019-10-04 17:41:28.724439: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "2019-10-04 17:41:28.725878: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\n",
      "2019-10-04 17:41:28.727100: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\n",
      "2019-10-04 17:41:28.727325: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\n",
      "2019-10-04 17:41:28.728672: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\n",
      "2019-10-04 17:41:28.729471: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\n",
      "2019-10-04 17:41:28.732386: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-04 17:41:28.739182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2019-10-04 17:41:28.739210: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "2019-10-04 17:41:30.158485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-04 17:41:30.158518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 \n",
      "2019-10-04 17:41:30.158526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y \n",
      "2019-10-04 17:41:30.158531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y \n",
      "2019-10-04 17:41:30.158538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y \n",
      "2019-10-04 17:41:30.158544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N \n",
      "2019-10-04 17:41:30.163221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15127 MB memory) -> physical GPU (device: 0, name: Tesla P100-DGXS-16GB, pci bus id: 0000:07:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:41:30.164546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15138 MB memory) -> physical GPU (device: 1, name: Tesla P100-DGXS-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:41:30.165791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15138 MB memory) -> physical GPU (device: 2, name: Tesla P100-DGXS-16GB, pci bus id: 0000:0e:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:41:30.167029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15138 MB memory) -> physical GPU (device: 3, name: Tesla P100-DGXS-16GB, pci bus id: 0000:0f:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:41:31.909400: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I1004 17:41:31.948915 140007069570880 session_manager.py:500] Running local_init_op.\n",
      "I1004 17:41:32.015150 140007069570880 session_manager.py:502] Done running local_init_op.\n",
      "2019-10-04 17:41:33.244791: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\n",
      "I1004 17:41:33.821089 140007069570880 run_squad.py:1373] Processing example: 0\n",
      "I1004 17:41:33.911081 140007069570880 error_handling.py:96] prediction_loop marked as finished\n",
      "I1004 17:41:33.911292 140007069570880 error_handling.py:96] prediction_loop marked as finished\n",
      "I1004 17:41:33.911444 140007069570880 run_squad.py:1388] -----------------------------\n",
      "I1004 17:41:33.911543 140007069570880 run_squad.py:1391] 0 Total Inference Time = 11.92 Inference Time W/O start up overhead = 1.31 Sentences processed = 8\n",
      "I1004 17:41:33.911634 140007069570880 run_squad.py:1392] 0 Inference Performance = 6.1243 sentences/sec\n",
      "I1004 17:41:33.911716 140007069570880 run_squad.py:1393] -----------------------------\n",
      "I1004 17:41:33.911850 140007069570880 run_squad.py:792] Writing predictions to: /workspace/bert/results/predictions.json\n",
      "I1004 17:41:33.911930 140007069570880 run_squad.py:793] Writing nbest to: /workspace/bert/results/nbest_predictions.json\n"
     ]
    }
   ],
   "source": [
    "# Ask BERT questions\n",
    "!python /workspace/bert/run_squad.py \\\n",
    "  --bert_config_file=$bert_config_file \\\n",
    "  --vocab_file=$vocab_file \\\n",
    "  --init_checkpoint=$init_checkpoint \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --do_predict=$do_predict \\\n",
    "  --predict_file=$predict_file \\\n",
    "  --predict_batch_size=$predict_batch_size \\\n",
    "  --doc_stride=$doc_stride \\\n",
    "  --max_seq_length=$max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELf0wtQ08Azw"
   },
   "source": [
    "### 4b. Display Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZ0OZclQ8Azw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Id</th><th>Question</th><th>Answer</th></tr><tr><td>Q1</td><td>What project put the first Americans into space?</td><td>Project Mercury</td></tr><tr><td>Q2</td><td>What program was created to carry out these projects and missions?</td><td>The Apollo program</td></tr><tr><td>Q3</td><td>What year did the first manned Apollo flight occur?</td><td>1968</td></tr><tr><td>Q4</td><td>What President is credited with the notion of putting Americans on the moon?</td><td>John F. Kennedy</td></tr><tr><td>Q5</td><td>Who did the U.S. collaborate with on an Earth orbit mission in 1975?</td><td>Soviet Union</td></tr><tr><td>Q6</td><td>How long did Project Apollo run?</td><td>1961 to 1972</td></tr><tr><td>Q7</td><td>What program helped develop space travel techniques that Project Apollo used?</td><td>Gemini missions</td></tr><tr><td>Q8</td><td>What space station supported three manned missions in 1973-1974?</td><td>Skylab</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results(predict_file, output_prediction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NH0Umn_e6Jsz"
   },
   "source": [
    "<details>\n",
    "  <summary><b>Click to reveal expected answers to the questions above</b></summary>\n",
    "  \n",
    "| Id | Question | Answer |\n",
    "|----|----------|--------|\n",
    "| Q1 | What project put the first Americans into space? | Project Mercury |\n",
    "| Q2 | What program was created to carry out these projects and missions? | The Apollo program |\n",
    "| Q3 | What year did the first manned Apollo flight occur? | 1968 |\n",
    "| Q4 | What President is credited with the notion of putting Americans on the moon?\t | John F. Kennedy |\n",
    "| Q5 | Who did the U.S. collaborate with on an Earth orbit mission in 1975? | Soviet Union |\n",
    "| Q6 | How long did Project Apollo run? | 1961 to 1972 |\n",
    "| Q7 | What program helped develop space travel techniques that Project Apollo used? | Gemini missions |\n",
    "| Q8 | What space station supported three manned missions in 1973-1974? | Skylab |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8EfbPm8Azz"
   },
   "source": [
    "## 5. Custom Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHWl7yus8Azz"
   },
   "source": [
    "Now that you are familiar with running QA Inference on BERT, you may want to try\n",
    "your own paragraphs and queries.\n",
    "\n",
    "\n",
    "1. Copy and paste your context from Wikipedia, news articles, etc. when prompted below\n",
    "2. Enter questions based on the context when prompted below.\n",
    "3. Run the inference script\n",
    "4. Display the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvnB1JUpWV_a"
   },
   "outputs": [],
   "source": [
    "predict_file = '/workspace/bert/config.qa/custom_input.json'\n",
    "num_questions = 3           # You can configure this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryd1akIpBaKz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your context here: Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; 23 June 1912 – 7 June 1954) was an English[6] mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[7] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[8][9][10] Turing is widely considered to be the father of theoretical computer science and artificial intelligence.[11] Despite these accomplishments, he was not fully recognised in his home country during his lifetime, due to his homosexuality, and because much of his work was covered by the Official Secrets Act.  During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine.  Turing played a pivotal role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic, and in so doing helped win the war.[12][13] Due to the problems of counterfactual history, it's hard to estimate what effect Ultra intelligence had on the war,[14] but at the upper end it has been estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives.[12]  After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, which was one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[15] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s.  Turing was prosecuted in 1952 for homosexual acts; the Labouchere Amendment of 1885 had mandated that \"gross indecency\" was a criminal offence in the UK. He accepted chemical castration treatment, with DES, as an alternative to prison. Turing died in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning.  In 2009, following an Internet campaign, British Prime Minister Gordon Brown made an official public apology on behalf of the British government for \"the appalling way he was treated\". Queen Elizabeth II granted Turing a posthumous pardon in 2013. The Alan Turing law is now an informal term for a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.[16]  On 15 July 2019 the Bank of England announced that Turing would be depicted on the United Kingdom's new £50 note.\n"
     ]
    }
   ],
   "source": [
    "# Create your own context to ask questions about.\n",
    "context = input(\"Paste your context here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEalqfQXnZDT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1/3: Who is Alan Turing?\n",
      "Question 2/3: What did he do?\n",
      "Question 3/3: Where did he work?\n"
     ]
    }
   ],
   "source": [
    "# Get questions from user input\n",
    "questions = [input(\"Question {}/{}: \".format(i+1, num_questions)) for i in range(num_questions)]\n",
    "# Format questions and write to JSON input file\n",
    "qinputs = [{ \"question\":q, \"id\":\"Q{}\".format(i+1)} for i,q in enumerate(questions)]\n",
    "write_input_file(context, qinputs, predict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_RbzPEGWeWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-04 17:42:08.018740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1004 17:42:09.129620 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/optimization.py:110: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1004 17:42:09.143753 139990286526272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/horovod-0.16.2-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:107: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "W1004 17:42:09.145139 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1409: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "W1004 17:42:09.145527 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1174: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1004 17:42:09.145627 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1174: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W1004 17:42:09.145733 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/modeling.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1004 17:42:09.146319 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1183: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W1004 17:42:09.227888 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1199: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1004 17:42:09.730068 139990286526272 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1004 17:42:09.730493 139990286526272 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5133717048>) includes params argument, but params are not passed to Estimator.\n",
      "I1004 17:42:09.731131 139990286526272 estimator.py:209] Using config: {'_model_dir': '/workspace/bert/results', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5127d46320>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "I1004 17:42:09.731536 139990286526272 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
      "W1004 17:42:09.731824 139990286526272 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n",
      "W1004 17:42:09.731940 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:266: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1004 17:42:09.733937 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1112: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W1004 17:42:09.784600 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:1354: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "I1004 17:42:09.784681 139990286526272 run_squad.py:1354] ***** Running predictions *****\n",
      "I1004 17:42:09.784742 139990286526272 run_squad.py:1355]   Num orig examples = 3\n",
      "I1004 17:42:09.785042 139990286526272 run_squad.py:1356]   Num split examples = 12\n",
      "I1004 17:42:09.785112 139990286526272 run_squad.py:1357]   Batch size = 8\n",
      "W1004 17:42:09.785203 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:731: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "I1004 17:42:09.785398 139990286526272 estimator.py:612] Could not find trained model in model_dir: /workspace/bert/results, running initialization to predict.\n",
      "W1004 17:42:09.805274 139990286526272 deprecation.py:323] From /workspace/bert/run_squad.py:776: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W1004 17:42:09.805377 139990286526272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W1004 17:42:09.806472 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:743: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W1004 17:42:09.809580 139990286526272 deprecation.py:323] From /workspace/bert/run_squad.py:750: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I1004 17:42:09.825074 139990286526272 estimator.py:1145] Calling model_fn.\n",
      "I1004 17:42:09.825195 139990286526272 tpu_estimator.py:2965] Running infer on CPU\n",
      "W1004 17:42:09.828354 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1004 17:42:09.829684 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/modeling.py:413: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1004 17:42:09.911925 139990286526272 deprecation.py:323] From /workspace/bert/modeling.py:675: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1004 17:42:14.175340 139990286526272 deprecation_wrapper.py:119] From /workspace/bert/run_squad.py:670: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "I1004 17:42:15.349575 139990286526272 estimator.py:1147] Done calling model_fn.\n",
      "W1004 17:42:15.603036 139990286526272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I1004 17:42:16.163598 139990286526272 monitored_session.py:240] Graph was finalized.\n",
      "2019-10-04 17:42:16.169261: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3198235000 Hz\n",
      "2019-10-04 17:42:16.169919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x743d320 executing computations on platform Host. Devices:\n",
      "2019-10-04 17:42:16.169938: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-10-04 17:42:16.172064: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-04 17:42:16.644922: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7515190 executing computations on platform CUDA. Devices:\n",
      "2019-10-04 17:42:16.644944: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:42:16.644950: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:42:16.644955: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:42:16.644960: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla P100-DGXS-16GB, Compute Capability 6.0\n",
      "2019-10-04 17:42:16.646154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:07:00.0\n",
      "2019-10-04 17:42:16.647045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:08:00.0\n",
      "2019-10-04 17:42:16.648029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:0e:00.0\n",
      "2019-10-04 17:42:16.649059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \n",
      "name: Tesla P100-DGXS-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:0f:00.0\n",
      "2019-10-04 17:42:16.649086: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "2019-10-04 17:42:16.650940: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\n",
      "2019-10-04 17:42:16.652507: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\n",
      "2019-10-04 17:42:16.652792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\n",
      "2019-10-04 17:42:16.654563: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\n",
      "2019-10-04 17:42:16.655673: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\n",
      "2019-10-04 17:42:16.659549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-04 17:42:16.667120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2019-10-04 17:42:16.667152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "2019-10-04 17:42:18.080335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-04 17:42:18.080365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 \n",
      "2019-10-04 17:42:18.080374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y \n",
      "2019-10-04 17:42:18.080379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y \n",
      "2019-10-04 17:42:18.080384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y \n",
      "2019-10-04 17:42:18.080389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N \n",
      "2019-10-04 17:42:18.085105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15127 MB memory) -> physical GPU (device: 0, name: Tesla P100-DGXS-16GB, pci bus id: 0000:07:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:42:18.086382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15138 MB memory) -> physical GPU (device: 1, name: Tesla P100-DGXS-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:42:18.087622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15138 MB memory) -> physical GPU (device: 2, name: Tesla P100-DGXS-16GB, pci bus id: 0000:0e:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:42:18.088840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15138 MB memory) -> physical GPU (device: 3, name: Tesla P100-DGXS-16GB, pci bus id: 0000:0f:00.0, compute capability: 6.0)\n",
      "2019-10-04 17:42:19.738443: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I1004 17:42:19.776933 139990286526272 session_manager.py:500] Running local_init_op.\n",
      "I1004 17:42:19.842164 139990286526272 session_manager.py:502] Done running local_init_op.\n",
      "2019-10-04 17:42:21.089302: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\n",
      "I1004 17:42:21.652874 139990286526272 run_squad.py:1373] Processing example: 0\n",
      "I1004 17:42:21.921473 139990286526272 error_handling.py:96] prediction_loop marked as finished\n",
      "I1004 17:42:21.921690 139990286526272 error_handling.py:96] prediction_loop marked as finished\n",
      "I1004 17:42:21.921820 139990286526272 run_squad.py:1388] -----------------------------\n",
      "I1004 17:42:21.921907 139990286526272 run_squad.py:1391] 0 Total Inference Time = 12.14 Inference Time W/O start up overhead = 1.49 Sentences processed = 16\n",
      "I1004 17:42:21.921993 139990286526272 run_squad.py:1392] 0 Inference Performance = 10.7314 sentences/sec\n",
      "I1004 17:42:21.922071 139990286526272 run_squad.py:1393] -----------------------------\n",
      "I1004 17:42:21.922402 139990286526272 run_squad.py:792] Writing predictions to: /workspace/bert/results/predictions.json\n",
      "I1004 17:42:21.922497 139990286526272 run_squad.py:793] Writing nbest to: /workspace/bert/results/nbest_predictions.json\n"
     ]
    }
   ],
   "source": [
    "# Ask BERT questions\n",
    "!python /workspace/bert/run_squad.py \\\n",
    "  --bert_config_file=$bert_config_file \\\n",
    "  --vocab_file=$vocab_file \\\n",
    "  --init_checkpoint=$init_checkpoint \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --do_predict=$do_predict \\\n",
    "  --predict_file=$predict_file \\\n",
    "  --predict_batch_size=$predict_batch_size \\\n",
    "  --doc_stride=$doc_stride \\\n",
    "  --max_seq_length=$max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMnxQZb_WiUN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Id</th><th>Question</th><th>Answer</th></tr><tr><td>Q1</td><td>Who is Alan Turing?</td><td>father of theoretical computer science and artificial intelligence</td></tr><tr><td>Q2</td><td>What did he do?</td><td>designed the Automatic Computing Engine</td></tr><tr><td>Q3</td><td>Where did he work?</td><td>Government Code and Cypher School</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results(predict_file, output_prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_colab_demo.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
