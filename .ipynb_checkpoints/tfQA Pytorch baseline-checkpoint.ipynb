{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/apex-master/apex/\n",
    "!pip install -v --no-cache-dir apex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --no-cache-dir transformers==2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version            \n",
      "------------------ -------------------\n",
      "attrs              19.3.0             \n",
      "backcall           0.1.0              \n",
      "bleach             3.1.4              \n",
      "certifi            2020.4.5.1         \n",
      "colorama           0.4.3              \n",
      "decorator          4.4.2              \n",
      "defusedxml         0.6.0              \n",
      "entrypoints        0.3                \n",
      "importlib-metadata 1.5.0              \n",
      "ipykernel          5.1.4              \n",
      "ipython            7.13.0             \n",
      "ipython-genutils   0.2.0              \n",
      "ipywidgets         7.5.1              \n",
      "jedi               0.17.0             \n",
      "Jinja2             2.11.2             \n",
      "jsonschema         3.2.0              \n",
      "jupyter            1.0.0              \n",
      "jupyter-client     6.1.3              \n",
      "jupyter-console    6.1.0              \n",
      "jupyter-core       4.6.3              \n",
      "libarchive-c       2.9                \n",
      "MarkupSafe         1.1.1              \n",
      "mistune            0.8.4              \n",
      "mkl-service        2.3.0              \n",
      "nb-conda           2.2.1              \n",
      "nb-conda-kernels   2.2.3              \n",
      "nbconvert          5.6.1              \n",
      "nbformat           5.0.6              \n",
      "notebook           6.0.3              \n",
      "numpy              1.11.3             \n",
      "pandas             0.23.4             \n",
      "pandocfilters      1.4.2              \n",
      "parso              0.7.0              \n",
      "pickleshare        0.7.5              \n",
      "pip                20.0.2             \n",
      "prometheus-client  0.7.1              \n",
      "prompt-toolkit     3.0.4              \n",
      "Pygments           2.6.1              \n",
      "pyrsistent         0.16.0             \n",
      "python-dateutil    2.8.1              \n",
      "pytz               2020.1             \n",
      "pywin32            227                \n",
      "pywinpty           0.5.7              \n",
      "pyzmq              18.1.1             \n",
      "qtconsole          4.7.3              \n",
      "QtPy               1.9.0              \n",
      "scipy              1.3.2              \n",
      "Send2Trash         1.5.0              \n",
      "setuptools         46.2.0.post20200511\n",
      "six                1.14.0             \n",
      "terminado          0.8.3              \n",
      "testpath           0.4.4              \n",
      "tornado            6.0.4              \n",
      "tqdm               4.46.0             \n",
      "traitlets          4.3.3              \n",
      "wcwidth            0.1.9              \n",
      "webencodings       0.5.1              \n",
      "wheel              0.34.2             \n",
      "widgetsnbextension 3.5.1              \n",
      "wincertstore       0.2                \n",
      "zipp               3.1.0              \n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Testing profiling\n",
    "\n",
    "for i in range(1,10):\n",
    "    a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from typing import Callable, Dict, List, Generator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json.json import JsonReader\n",
    "#from pandas.io.json._json import JsonReader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from apex import amp\n",
    "#from transformers import BertTokenizer, AdamW, WarmupLinearSchedule, BertModel, BertPreTrainedModel\n",
    "from transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: /data/sv/CS230_Spring-2020/Guanshuo_TFQA_1stplace/input/rekha_data/simplified-nq-train_5000.jsonl: No such file or directory\n",
      "100 /data/rekha/rekha_data/train_head6k_tail100_total100.jsonl\n"
     ]
    }
   ],
   "source": [
    "#DATA_PATH = '/data/sv/CS230_Spring-2020/Guanshuo_TFQA_1stplace/input/rekha_data/simplified-nq-train_100.jsonl'\n",
    "#EVAL_DATA_PATH = '/data/sv/CS230_Spring-2020/Guanshuo_TFQA_1stplace/input/rekha_data/simplified-nq-valid_50.jsonl'\n",
    "\n",
    "DATA_PATH = '/data/sv/CS230_Spring-2020/Guanshuo_TFQA_1stplace/input/rekha_data/simplified-nq-train_5000.jsonl'\n",
    "#EVAL_DATA_PATH = '/data/sv/CS230_Spring-2020/Guanshuo_TFQA_1stplace/input/rekha_data/simplified-nq-valid_100.jsonl'\n",
    "EVAL_DATA_PATH = '/data/rekha/rekha_data/train_head6k_tail100_total100.jsonl'\n",
    "\n",
    "TRAIN_SIZE = 5000\n",
    "VALID_SIZE = 100\n",
    "\n",
    "!wc -l $DATA_PATH\n",
    "!wc -l $EVAL_DATA_PATH\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = Path('../input/tensorflow2-question-answering/')\n",
    "#DATA_PATH = DATA_DIR / 'simplified-nq-train.jsonl'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "seed = 1029\n",
    "valid_size = VALID_SIZE\n",
    "train_size = TRAIN_SIZE\n",
    "\n",
    "chunksize = 1000\n",
    "max_seq_len = 384\n",
    "max_question_len = 64\n",
    "doc_stride = 128\n",
    "\n",
    "num_labels = 5\n",
    "n_epochs = 1\n",
    "lr = 2e-5\n",
    "warmup = 0.05\n",
    "batch_size = 16\n",
    "accumulation_steps = 4\n",
    "\n",
    "bert_model = 'bert-base-uncased'\n",
    "do_lower_case = 'uncased' in bert_model\n",
    "device = torch.device('cuda')\n",
    "\n",
    "output_model_file = 'bert_pytorch.bin'\n",
    "output_optimizer_file = 'bert_pytorch_optimizer.bin'\n",
    "output_amp_file = 'bert_pytorch_amp.bin'\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example(object):\n",
    "    example_id: int\n",
    "    candidates: List[Dict]\n",
    "    annotations: Dict\n",
    "    doc_start: int\n",
    "    question_len: int\n",
    "    tokenized_to_original_index: List[int]\n",
    "    input_ids: List[int]\n",
    "    start_position: int\n",
    "    end_position: int\n",
    "    class_label: str\n",
    "\n",
    "        \n",
    "def convert_data(\n",
    "    line: str,\n",
    "    tokenizer: BertTokenizer,\n",
    "    max_seq_len: int,\n",
    "    max_question_len: int,\n",
    "    doc_stride: int\n",
    ") -> List[Example]:\n",
    "    \"\"\"Convert dictionary data into list of training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line : str\n",
    "        Training data.\n",
    "    tokenizer : transformers.BertTokenizer\n",
    "        Tokenizer for encoding texts into ids.\n",
    "    max_seq_len : int\n",
    "        Maximum input sequence length.\n",
    "    max_question_len : int\n",
    "        Maximum input question length.\n",
    "    doc_stride : int\n",
    "        When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def _find_short_range(short_answers: List[Dict]) -> Tuple[int, int]:\n",
    "        answers = pd.DataFrame(short_answers)\n",
    "        start_min = answers['start_token'].min()\n",
    "        end_max = answers['end_token'].max()\n",
    "        return start_min, end_max\n",
    "\n",
    "    # model input\n",
    "    data = json.loads(line)\n",
    "    doc_words = data['document_text'].split()\n",
    "    question_tokens = tokenizer.tokenize(data['question_text'])[:max_question_len]\n",
    "\n",
    "    # tokenized index of i-th original token corresponds to original_to_tokenized_index[i]\n",
    "    # if a token in original text is removed, its tokenized index indicates next token\n",
    "    original_to_tokenized_index = []\n",
    "    tokenized_to_original_index = []\n",
    "    all_doc_tokens = []  # tokenized document text\n",
    "    for i, word in enumerate(doc_words):\n",
    "        original_to_tokenized_index.append(len(all_doc_tokens))\n",
    "        if re.match(r'<.+>', word):  # remove paragraph tag\n",
    "            continue\n",
    "        sub_tokens = tokenizer.tokenize(word)\n",
    "        for sub_token in sub_tokens:\n",
    "            tokenized_to_original_index.append(i)\n",
    "            all_doc_tokens.append(sub_token)\n",
    "\n",
    "    # model output: (class_label, start_position, end_position)\n",
    "    annotations = data['annotations'][0]\n",
    "    if annotations['yes_no_answer'] in ['YES', 'NO']:\n",
    "        class_label = annotations['yes_no_answer'].lower()\n",
    "        start_position = annotations['long_answer']['start_token']\n",
    "        end_position = annotations['long_answer']['end_token']\n",
    "    elif annotations['short_answers']:\n",
    "        class_label = 'short'\n",
    "        start_position, end_position = _find_short_range(annotations['short_answers'])\n",
    "    elif annotations['long_answer']['candidate_index'] != -1:\n",
    "        class_label = 'long'\n",
    "        start_position = annotations['long_answer']['start_token']\n",
    "        end_position = annotations['long_answer']['end_token']\n",
    "    else:\n",
    "        class_label = 'unknown'\n",
    "        start_position = -1\n",
    "        end_position = -1\n",
    "\n",
    "    # convert into tokenized index\n",
    "    if start_position != -1 and end_position != -1:\n",
    "        start_position = original_to_tokenized_index[start_position]\n",
    "        end_position = original_to_tokenized_index[end_position]\n",
    "\n",
    "    # make sure at least one object in `examples`\n",
    "    examples = []\n",
    "    max_doc_len = max_seq_len - len(question_tokens) - 3  # [CLS], [SEP], [SEP]\n",
    "\n",
    "    # take chunks with a stride of `doc_stride`\n",
    "    for doc_start in range(0, len(all_doc_tokens), doc_stride):\n",
    "        doc_end = doc_start + max_doc_len\n",
    "        # if truncated document does not contain annotated range\n",
    "        if not (doc_start <= start_position and end_position <= doc_end):\n",
    "            start, end, label = -1, -1, 'unknown'\n",
    "        else:\n",
    "            start = start_position - doc_start + len(question_tokens) + 2\n",
    "            end = end_position - doc_start + len(question_tokens) + 2\n",
    "            label = class_label\n",
    "\n",
    "        assert -1 <= start < max_seq_len, f'start position is out of range: {start}'\n",
    "        assert -1 <= end < max_seq_len, f'end position is out of range: {end}'\n",
    "\n",
    "        doc_tokens = all_doc_tokens[doc_start:doc_end]\n",
    "        input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + doc_tokens + ['[SEP]']\n",
    "        examples.append(\n",
    "            Example(\n",
    "                example_id=data['example_id'],\n",
    "                candidates=data['long_answer_candidates'],\n",
    "                annotations=annotations,\n",
    "                doc_start=doc_start,\n",
    "                question_len=len(question_tokens),\n",
    "                tokenized_to_original_index=tokenized_to_original_index,\n",
    "                input_ids=tokenizer.convert_tokens_to_ids(input_tokens),\n",
    "                start_position=start,\n",
    "                end_position=end,\n",
    "                class_label=label\n",
    "        ))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonChunkReader(JsonReader):\n",
    "    \"\"\"JsonReader provides an interface for reading in a JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath_or_buffer: str,\n",
    "        convert_data: Callable[[str], List[Example]],\n",
    "        orient: str = None,\n",
    "        typ: str = 'frame',\n",
    "        dtype: bool = None,\n",
    "        convert_axes: bool = None,\n",
    "        convert_dates: bool = True,\n",
    "        keep_default_dates: bool = True,\n",
    "        numpy: bool = False,\n",
    "        precise_float: bool = False,\n",
    "        date_unit: str = None,\n",
    "        encoding: str = None,\n",
    "        lines: bool = True,\n",
    "        chunksize: int = 2000,\n",
    "        compression: str = None,\n",
    "    ):\n",
    "        super(JsonChunkReader, self).__init__(\n",
    "            str(filepath_or_buffer),\n",
    "            orient=orient, typ=typ, dtype=dtype,\n",
    "            convert_axes=convert_axes,\n",
    "            convert_dates=convert_dates,\n",
    "            keep_default_dates=keep_default_dates,\n",
    "            numpy=numpy, precise_float=precise_float,\n",
    "            date_unit=date_unit, encoding=encoding,\n",
    "            lines=lines, chunksize=chunksize,\n",
    "            compression=compression\n",
    "        )\n",
    "        self.convert_data = convert_data\n",
    "        \n",
    "    def __next__(self):\n",
    "        lines = list(itertools.islice(self.data, self.chunksize))\n",
    "        #for line in lines:\n",
    "            #print(line)\n",
    "        #print('Length of lines',len(lines), 'chunksize', self.chunksize)\n",
    "        if lines:\n",
    "            with Pool(2) as p: \n",
    "                obj = p.map(self.convert_data, lines)\n",
    "                #print('Length of obj', len(obj))\n",
    "            return obj\n",
    "\n",
    "        self.close()\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : list of Example\n",
    "        The whole Dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, examples: List[Example]):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        annotated = list(\n",
    "            filter(lambda example: example.class_label != 'unknown', self.examples[index]))\n",
    "        if len(annotated) == 0:\n",
    "            return random.choice(self.examples[index])\n",
    "        return random.choice(annotated)\n",
    "\n",
    "    \n",
    "def collate_fn(examples: List[Example]) -> List[List[torch.Tensor]]:\n",
    "    # input tokens\n",
    "    max_len = max([len(example.input_ids) for example in examples])\n",
    "    tokens = np.zeros((len(examples), max_len), dtype=np.int64)\n",
    "    token_type_ids = np.ones((len(examples), max_len), dtype=np.int64)\n",
    "    for i, example in enumerate(examples):\n",
    "        row = example.input_ids\n",
    "        tokens[i, :len(row)] = row\n",
    "        token_type_id = [0 if i <= row.index(102) else 1\n",
    "                         for i in range(len(row))]  # 102 corresponds to [SEP]\n",
    "        token_type_ids[i, :len(row)] = token_type_id\n",
    "    attention_mask = tokens > 0\n",
    "    inputs = [torch.from_numpy(tokens),\n",
    "              torch.from_numpy(attention_mask),\n",
    "              torch.from_numpy(token_type_ids)]\n",
    "\n",
    "    # output labels\n",
    "    all_labels = ['long', 'no', 'short', 'unknown', 'yes']\n",
    "    start_positions = np.array([example.start_position for example in examples])\n",
    "    end_positions = np.array([example.end_position for example in examples])\n",
    "    class_labels = [all_labels.index(example.class_label) for example in examples]\n",
    "    start_positions = np.where(start_positions >= max_len, -1, start_positions)\n",
    "    end_positions = np.where(end_positions >= max_len, -1, end_positions)\n",
    "    labels = [torch.LongTensor(start_positions),\n",
    "              torch.LongTensor(end_positions),\n",
    "              torch.LongTensor(class_labels)]\n",
    "\n",
    "    return [inputs, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for QA and classification tasks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : transformers.BertConfig. Configuration class for BERT.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    start_logits : torch.Tensor with shape (batch_size, sequence_size).\n",
    "        Starting scores of each tokens.\n",
    "    end_logits : torch.Tensor with shape (batch_size, sequence_size).\n",
    "        Ending scores of each tokens.\n",
    "    classifier_logits : torch.Tensor with shape (batch_size, num_classes).\n",
    "        Classification scores of each labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)  # start/end\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids, \n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        # predict start & end position\n",
    "        qa_logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "    \n",
    "        # classification\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        classifier_logits = self.classifier(pooled_output)\n",
    "\n",
    "        return start_logits, end_logits, classifier_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds, class_preds = preds\n",
    "    start_labels, end_labels, class_labels = labels\n",
    "    \n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    class_loss = nn.CrossEntropyLoss()(class_preds, class_labels)\n",
    "    return start_loss + end_loss + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_optimization_steps= 78\n",
      "num_warmup_steps 3\n",
      "Rekha train_size= 5000\n",
      "Rekha total=int(np.ceil(train_size / chunksize))= 5\n",
      "Rekha DATA_PATH /data/sv/CS230_Spring-2020/Guanshuo_TFQA_1stplace/input/rekha_data/simplified-nq-train_5000.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a8f5f5441645be90a27040c3abea8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ec2-user/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-23-4a6feb345350>\", line 45, in convert_data\n    data = json.loads(line)\n  File \"/home/ec2-user/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/json/__init__.py\", line 354, in loads\n    return _default_decoder.decode(s)\n  File \"/home/ec2-user/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/json/decoder.py\", line 339, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/home/ec2-user/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-13e76aefe19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rekha DATA_PATH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#print('len(data_reader)',len(data_reader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-bafce8a89d44>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;31m#print('Length of obj', len(obj))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmapstar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4a6feb345350>\u001b[0m in \u001b[0;36mconvert_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# model input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdoc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mquestion_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_question_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m()\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(bert_model, num_labels=5)\n",
    "model = model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "num_train_optimization_steps = int(n_epochs * train_size / batch_size / accumulation_steps)\n",
    "print('num_train_optimization_steps=', num_train_optimization_steps)\n",
    "num_warmup_steps = int(num_train_optimization_steps * warmup)\n",
    "print('num_warmup_steps', num_warmup_steps)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
    "#scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_train_optimization_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps = num_train_optimization_steps)`\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "model.zero_grad()\n",
    "model = model.train()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
    "convert_func = functools.partial(convert_data,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 max_question_len=max_question_len,\n",
    "                                 doc_stride=doc_stride)\n",
    "data_reader = JsonChunkReader(DATA_PATH, convert_func, chunksize=chunksize)\n",
    "\n",
    "global_step = 0\n",
    "print('Rekha train_size=', train_size)\n",
    "print('Rekha total=int(np.ceil(train_size / chunksize))=' , int(np.ceil(train_size / chunksize)))\n",
    "print('Rekha DATA_PATH', DATA_PATH)\n",
    "#print('len(data_reader)',len(data_reader))\n",
    "for examples in tqdm(data_reader, total=int(np.ceil(train_size / chunksize))): \n",
    "    train_dataset = TextDataset(examples)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, attention_mask, token_type_ids = x_batch\n",
    "        y_batch = (y.to(device) for y in y_batch)\n",
    "\n",
    "        y_pred = model(x_batch.to(device),\n",
    "                       attention_mask=attention_mask.to(device),\n",
    "                       token_type_ids=token_type_ids.to(device))\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        if (global_step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "    if (time.time() - start_time) / 3600 > 7:\n",
    "        break\n",
    "\n",
    "del examples, train_dataset, train_loader\n",
    "gc.collect()\n",
    "\n",
    "torch.save(model.state_dict(), output_model_file)\n",
    "torch.save(optimizer.state_dict(), output_optimizer_file)\n",
    "torch.save(amp.state_dict(), output_amp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'trained {global_step * batch_size} samples')\n",
    "print(f'training time: {(time.time() - start_time) / 3600:.1f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(examples: List[Example]) -> Tuple[List[torch.Tensor], List[Example]]:\n",
    "    # input tokens\n",
    "    max_len = max([len(example.input_ids) for example in examples])\n",
    "    tokens = np.zeros((len(examples), max_len), dtype=np.int64)\n",
    "    token_type_ids = np.ones((len(examples), max_len), dtype=np.int64)\n",
    "    for i, example in enumerate(examples):\n",
    "        row = example.input_ids\n",
    "        tokens[i, :len(row)] = row\n",
    "        token_type_id = [0 if i <= row.index(102) else 1\n",
    "                         for i in range(len(row))]  # 102 corresponds to [SEP]\n",
    "        token_type_ids[i, :len(row)] = token_type_id\n",
    "    attention_mask = tokens > 0\n",
    "    inputs = [torch.from_numpy(tokens),\n",
    "              torch.from_numpy(attention_mask),\n",
    "              torch.from_numpy(token_type_ids)]\n",
    "\n",
    "    return inputs, examples\n",
    "\n",
    "\n",
    "def eval_model(\n",
    "    model: nn.Module,\n",
    "    valid_loader: DataLoader,\n",
    "    device: torch.device = torch.device('cuda')\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute validation score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model for prediction.\n",
    "    valid_loader : DataLoader\n",
    "        Data loader of validation data.\n",
    "    device : torch.device, optional\n",
    "        Device for computation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Scores of validation data.\n",
    "        `long_score`: score of long answers\n",
    "        `short_score`: score of short answers\n",
    "        `overall_score`: score of the competition metric\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = Result()\n",
    "        for inputs, examples in tqdm(valid_loader):\n",
    "            input_ids, attention_mask, token_type_ids = inputs\n",
    "            y_preds = model(input_ids.to(device),\n",
    "                            attention_mask.to(device),\n",
    "                            token_type_ids.to(device))\n",
    "            \n",
    "            start_preds, end_preds, class_preds = (p.detach().cpu() for p in y_preds)\n",
    "            start_logits, start_index = torch.max(start_preds, dim=1)\n",
    "            end_logits, end_index = torch.max(end_preds, dim=1)\n",
    "\n",
    "            # span logits minus the cls logits seems to be close to the best\n",
    "            cls_logits = start_preds[:, 0] + end_preds[:, 0]  # '[CLS]' logits\n",
    "            logits = start_logits + end_logits - cls_logits  # (batch_size,)\n",
    "            indices = torch.stack((start_index, end_index)).transpose(0, 1)  # (batch_size, 2)\n",
    "            result.update(examples, logits.numpy(), indices.numpy(), class_preds.numpy())\n",
    "\n",
    "    return result.score()\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    \"\"\"Stores results of all test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.examples = {}\n",
    "        self.results = {}\n",
    "        self.best_scores = defaultdict(float)\n",
    "        self.class_labels = ['LONG', 'NO', 'SHORT', 'UNKNOWN', 'YES']\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_valid_index(example: Example, index: List[int]) -> bool:\n",
    "        \"\"\"Return whether valid index or not.\n",
    "        \"\"\"\n",
    "        start_index, end_index = index\n",
    "        if start_index > end_index:\n",
    "            return False\n",
    "        if start_index <= example.question_len + 2:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        examples: List[Example],\n",
    "        logits: torch.Tensor,\n",
    "        indices: torch.Tensor,\n",
    "        class_preds: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Update batch objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : list of Example\n",
    "        logits : np.ndarray with shape (batch_size,)\n",
    "            Scores of each examples..\n",
    "        indices : np.ndarray with shape (batch_size, 2)\n",
    "            `start_index` and `end_index` pairs of each examples.\n",
    "        class_preds : np.ndarray with shape (batch_size, num_classes)\n",
    "            Class predicition scores of each examples.\n",
    "        \"\"\"\n",
    "        for i, example in enumerate(examples):\n",
    "            if self.is_valid_index(example, indices[i]) and \\\n",
    "               self.best_scores[example.example_id] < logits[i]:\n",
    "                self.best_scores[example.example_id] = logits[i]\n",
    "                self.examples[example.example_id] = example\n",
    "                self.results[example.example_id] = [\n",
    "                    example.doc_start, indices[i], class_preds[i]]\n",
    "\n",
    "    def _generate_predictions(self) -> Generator[Dict, None, None]:\n",
    "        \"\"\"Generate predictions of each examples.\n",
    "        \"\"\"\n",
    "        for example_id in self.results.keys():\n",
    "            doc_start, index, class_pred = self.results[example_id]\n",
    "            example = self.examples[example_id]\n",
    "            tokenized_to_original_index = example.tokenized_to_original_index\n",
    "            short_start_index = tokenized_to_original_index[doc_start + index[0]]\n",
    "            short_end_index = tokenized_to_original_index[doc_start + index[1]]\n",
    "            long_start_index = -1\n",
    "            long_end_index = -1\n",
    "            for candidate in example.candidates:\n",
    "                if candidate['start_token'] <= short_start_index and \\\n",
    "                   short_end_index <= candidate['end_token']:\n",
    "                    long_start_index = candidate['start_token']\n",
    "                    long_end_index = candidate['end_token']\n",
    "                    break\n",
    "            yield {\n",
    "                'example': example,\n",
    "                'long_answer': [long_start_index, long_end_index],\n",
    "                'short_answer': [short_start_index, short_end_index],\n",
    "                'yes_no_answer': class_pred\n",
    "            }\n",
    "\n",
    "    def end(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Get predictions in submission format.\n",
    "        \"\"\"\n",
    "        preds = {}\n",
    "        for pred in self._generate_predictions():\n",
    "            example = pred['example']\n",
    "            long_start_index, long_end_index = pred['long_answer']\n",
    "            short_start_index, short_end_index = pred['short_answer']\n",
    "            class_pred = pred['yes_no_answer']\n",
    "\n",
    "            long_answer = f'{long_start_index}:{long_end_index}' if long_start_index != -1 else np.nan\n",
    "            short_answer = f'{short_start_index}:{short_end_index}'\n",
    "            class_pred = self.class_labels[class_pred.argmax()]\n",
    "            short_answer += ' ' + class_pred if class_pred in ['YES', 'NO'] else ''\n",
    "            preds[f'{example.example_id}_long'] = long_answer\n",
    "            preds[f'{example.example_id}_short'] = short_answer\n",
    "        return preds\n",
    "\n",
    "    def score(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate score of all examples.\n",
    "        \"\"\"\n",
    "\n",
    "        def _safe_divide(x: int, y: int) -> float:\n",
    "            \"\"\"Compute x / y, but return 0 if y is zero.\n",
    "            \"\"\"\n",
    "            if y == 0:\n",
    "                return 0.\n",
    "            else:\n",
    "                return x / y\n",
    "\n",
    "        def _compute_f1(answer_stats: List[List[bool]]) -> float:\n",
    "            \"\"\"Computes F1, precision, recall for a list of answer scores.\n",
    "            \"\"\"\n",
    "            has_answer, has_pred, is_correct = list(zip(*answer_stats))\n",
    "            precision = _safe_divide(sum(is_correct), sum(has_pred))\n",
    "            recall = _safe_divide(sum(is_correct), sum(has_answer))\n",
    "            print('precision=' , precision)\n",
    "            print('recall=', recall)\n",
    "            f1 = _safe_divide(2 * precision * recall, precision + recall)\n",
    "            return f1\n",
    "\n",
    "        long_scores = []\n",
    "        short_scores = []\n",
    "        for pred in self._generate_predictions():\n",
    "            example = pred['example']\n",
    "            long_pred = pred['long_answer']\n",
    "            short_pred = pred['short_answer']\n",
    "            class_pred = pred['yes_no_answer']\n",
    "            yes_no_label = self.class_labels[class_pred.argmax()]\n",
    "            \n",
    "            if(DEBUG):\n",
    "                print(example)\n",
    "                print('long_pred=',long_pred)\n",
    "                print('short_pred=',short_pred)\n",
    "\n",
    "            # long score\n",
    "            long_label = example.annotations['long_answer']\n",
    "            has_answer = long_label['candidate_index'] != -1\n",
    "            has_pred = long_pred[0] != -1 and long_pred[1] != -1\n",
    "            is_correct = False\n",
    "            if long_label['start_token'] == long_pred[0] and \\\n",
    "               long_label['end_token'] == long_pred[1]:\n",
    "                is_correct = True\n",
    "            long_scores.append([has_answer, has_pred, is_correct])\n",
    "\n",
    "            # short score\n",
    "            short_labels = example.annotations['short_answers']\n",
    "            class_pred = example.annotations['yes_no_answer']\n",
    "            has_answer = yes_no_label != 'NONE' or len(short_labels) != 0\n",
    "            has_pred = class_pred != 'NONE' or (short_pred[0] != -1 and short_pred[1] != -1)\n",
    "            is_correct = False\n",
    "            if class_pred in ['YES', 'NO']:\n",
    "                is_correct = yes_no_label == class_pred\n",
    "            else:\n",
    "                for short_label in short_labels:\n",
    "                    if short_label['start_token'] == short_pred[0] and \\\n",
    "                       short_label['end_token'] == short_pred[1]:\n",
    "                        is_correct = True\n",
    "                        break\n",
    "            short_scores.append([has_answer, has_pred, is_correct])\n",
    "\n",
    "        print('Long Answer')\n",
    "        long_score = _compute_f1(long_scores)\n",
    "        print('Short Answer')\n",
    "        short_score = _compute_f1(short_scores)\n",
    "        return {\n",
    "            'long_score': long_score,\n",
    "            'short_score': short_score,\n",
    "            'overall_score': (long_score + short_score) / 2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rekha adding\n",
    "data_reader = JsonChunkReader(EVAL_DATA_PATH, convert_func, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_start_time = time.time()\n",
    "valid_data = next(data_reader)\n",
    "valid_data = list(itertools.chain.from_iterable(valid_data))\n",
    "valid_dataset = Subset(valid_data, range(len(valid_data)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=eval_collate_fn)\n",
    "valid_scores = eval_model(model, valid_loader, device=device)\n",
    "\n",
    "print(f'calculate validation score done in {(time.time() - eval_start_time) / 60:.1f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_score = valid_scores['long_score']\n",
    "short_score = valid_scores['short_score']\n",
    "overall_score = valid_scores['overall_score']\n",
    "print('validation scores:')\n",
    "print(f'\\tlong score    : {long_score:.4f}')\n",
    "print(f'\\tshort score   : {short_score:.4f}')\n",
    "print(f'\\toverall score : {overall_score:.4f}')\n",
    "print(f'all process done in {(time.time() - start_time) / 3600:.1f} hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l $DATA_PATH\n",
    "!wc -l $EVAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jupyter] *",
   "language": "python",
   "name": "conda-env-.conda-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
